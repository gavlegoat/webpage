{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1J7qTm1RinT"
   },
   "source": [
    "## Coding 09: Keypose Estimation\n",
    "\n",
    "In this exercise, we'll do keypose estimation on the [Leeds Sport Pose Dataset (LSP)](https://sam.johnson.io/research/lsp.html).\n",
    "\n",
    "Keypose estimation is a detection task - and one of the most popular tasks in core computer vision. There's a lot of different ways to tackle this task and we'll explore the single-stage detection methods to do this (YOLO, RetinaNet, CenterNet). We'll use a dataset of sports athletes with their corresponding joint labels,  and the goal is to train a network that outputs the xy position of each joint.\n",
    "\n",
    "<img src=\"https://www.ee.cuhk.edu.hk/~xgwang/StructureFeature/QualitiveRes.png\" width=720px/>\n",
    "\n",
    "Image: Structured Feature Learning for Pose Estimation\n",
    ", Chu et al. CVPR 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF6uiLNYSnSU"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Run this cell to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zudQV9Q6usIl"
   },
   "outputs": [],
   "source": [
    "!rm -f lsp_dataset_original.zip\n",
    "!wget http://sam.johnson.io/research/lsp_dataset_original.zip\n",
    "!unzip -oq lsp_dataset_original.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppgc3qQ3Oyga"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset yields pairs of (image, label): \n",
    " * The image is a [3, 128, 128] float tensor of an athlete doing something cool.  \n",
    " * The label is a [14, 2] -1 to 1 float tensor of each joint (normalized coordinates)\n",
    "\n",
    "#### Labels\n",
    "\n",
    "There are 14 different joints, `label[0]` is a xy coordinate of joint `0`, maybe the head or right wrist.\n",
    "\n",
    "### Think about...\n",
    "\n",
    "How would you implement additional transforms such as  \n",
    "\n",
    "* RandomCrop\n",
    "* RandomHorizontalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcJR3-wXS0hp"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "EDGES = [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5],  [6, 7], [7, 8],\n",
    "         [8, 12], [12, 9], [9, 10], [10, 11], [12, 13], [2, 8], [3, 9]]\n",
    "\n",
    "class MyResize():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        \"\"\"\n",
    "        Labels come in in pixel coordinates - \n",
    "\n",
    "        label[:, 0] is the x coordinate, in the range [0, width]\n",
    "        label[:, 1] is the y coordinate, in the range [0, height]\n",
    "\n",
    "        we convert these to [-1, 1] for several reasons - can you think of any?\n",
    "        \"\"\"\n",
    "        xs = (label[:, 0] / image.width - 0.5) * 2.0\n",
    "        ys = (label[:, 1] / image.height - 0.5) * 2.0\n",
    "\n",
    "        image = torchvision.transforms.functional.resize(image, self.size)\n",
    "        label = np.stack((xs, ys), 1)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class LSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        dataset_dir = pathlib.Path(dataset_dir)\n",
    "\n",
    "        self.transform = MyResize((128, 128))\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "        self.images = list(sorted((dataset_dir / 'images').glob('*.jpg')))\n",
    "        self.labels = loadmat(str(dataset_dir / 'joints.mat'))['joints'].transpose(2, 1, 0).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images[i])\n",
    "        label = self.labels[i]\n",
    "\n",
    "        image, label = self.transform(image, label)\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def fetch_dataloader(batch_size, transform=None, split='train'):\n",
    "    \"\"\"\n",
    "    Loads data from disk and returns a data_loader.\n",
    "    A DataLoader is similar to a list of (image, label) tuples.\n",
    "    \"\"\"\n",
    "    data = LSPDataset(dataset_dir='.')\n",
    "\n",
    "    # Custom train/val split.\n",
    "    if split == 'train':\n",
    "        indices = [i for i in range(len(data)) if (i%10 > 1)]\n",
    "    elif split == 'val':\n",
    "        indices = [i for i in range(len(data)) if (i%10 == 1)]\n",
    "    else:\n",
    "        indices = [i for i in range(len(data)) if (i%10 == 0)]\n",
    "\n",
    "    data = torch.utils.data.Subset(data, indices)\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=split=='train', num_workers=0)\n",
    "    return loader\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_sample(image, label):\n",
    "    image_numpy = (255 * image.cpu().numpy().transpose(1, 2, 0)).astype(np.uint8)\n",
    "\n",
    "    debug = Image.fromarray(image_numpy)\n",
    "    draw = ImageDraw.Draw(debug)\n",
    "\n",
    "    for i, j in EDGES:\n",
    "        xi = (label[i, 0].item() / 2.0 + 0.5) * debug.width\n",
    "        yi = (label[i, 1].item() / 2.0 + 0.5) * debug.height\n",
    "\n",
    "        xj = (label[j, 0].item() / 2.0 + 0.5) * debug.width\n",
    "        yj = (label[j, 1].item() / 2.0 + 0.5) * debug.height\n",
    "\n",
    "        draw.line((xi, yi, xj, yj), fill=(255, 0, 0))\n",
    "\n",
    "    result = np.array(debug)\n",
    "\n",
    "    return torchvision.transforms.functional.to_tensor(result)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_train = fetch_dataloader(32, split='train')\n",
    "data_val = fetch_dataloader(32, split='val')\n",
    "data_test = fetch_dataloader(32, split='test')\n",
    "\n",
    "num_to_show = 4\n",
    "vis = []\n",
    "vis_data = next(iter(data_train))\n",
    "for i in range(num_to_show):\n",
    "    image, label = vis_data[0][i], vis_data[1][i]\n",
    "    vis.append(visualize_sample(image, label))\n",
    "\n",
    "vis = torchvision.utils.make_grid(vis, nrow=num_to_show)\n",
    "plt.figure(figsize=(num_to_show * 5, 5))\n",
    "plt.imshow(vis.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsjUZNGoNbJs"
   },
   "source": [
    "## TensorBoard Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaoyJOLrNaff"
   },
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard as tb\n",
    "import tempfile\n",
    "\n",
    "log_dir = tempfile.mkdtemp()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} --reload_interval 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vhhW0eCNiKX"
   },
   "source": [
    "## Model and Training\n",
    "\n",
    "Some questions to wonder about -\n",
    "\n",
    "* how should the network be structured?\n",
    "* what loss should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3JQBXDGXB6J"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SoftArgmax(torch.nn.Module):\n",
    "    def __init__(self, input_channels=3, num_joints=14):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(input_channels, num_joints, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        hmap = torch.nn.functional.softmax(x.view(*x.shape[:2], -1), dim=-1).view(*x.shape)\n",
    "        xs = torch.linspace(-1, 1, hmap.shape[-1], device=x.device)\n",
    "        ys = torch.linspace(-1, 1, hmap.shape[-2], device=x.device)\n",
    "        pred_x = (hmap * xs[None, None, None]).sum(dim=(2,3))\n",
    "        pred_y = (hmap * ys[None, None, :, None]).sum(dim=(2,3))\n",
    "        \n",
    "        return torch.stack((pred_x, pred_y), dim=-1)\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_channels=3, num_joints=14):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(input_channels, 16, 5, padding=2, stride=2)\n",
    "        self.soft_argmax = SoftArgmax(16, num_joints)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Predict joint positions from the image.\n",
    "\n",
    "        Input: \n",
    "            x (float tensor N x 3 x 128 x 128): input image of a person\n",
    "        Output:\n",
    "            y (float tensor N x 14 x 2): xy coordinates of joint positions\n",
    "        \"\"\"\n",
    "        x = self.conv1(self.input_norm(x))\n",
    "        return self.soft_argmax(torch.relu(x))\n",
    "\n",
    "\n",
    "def visualize(logger, image, pred, label, global_step):\n",
    "    image_preds = list()\n",
    "    image_labels = list()\n",
    "\n",
    "    for _image, _pred, _label in zip(image, pred, label):\n",
    "        image_pred = visualize_sample(_image, _pred)\n",
    "        image_preds.append(image_pred)\n",
    "\n",
    "        image_label = visualize_sample(_image, _label)\n",
    "        image_labels.append(image_label)\n",
    "\n",
    "    logger.add_image('pred', torchvision.utils.make_grid(image_preds), global_step=global_step)\n",
    "    logger.add_image('label', torchvision.utils.make_grid(image_labels), global_step=global_step)\n",
    "\n",
    "\n",
    "def calculate_pdj(pred, label):\n",
    "    widths = torch.max(label[:, :, 0], 1)[0] - torch.min(label[:, :, 0], 1)[0]\n",
    "    heights = torch.max(label[:, :, 1], 1)[0] - torch.min(label[:, :, 1], 1)[0]\n",
    "    diagonals = torch.sqrt(widths ** 2 + heights ** 2)[:, None]\n",
    "    d = torch.sqrt((pred[:, :, 0] - label[:, :, 0]) ** 2 + (pred[:, :, 1] - label[:, :, 1]) ** 2)\n",
    "    return (d < 0.05 * diagonals).float().mean()\n",
    "\n",
    "\n",
    "def train(model, device, lr=1e-3, epochs=10):\n",
    "    # Setting up the tensorboard logger\n",
    "    logger = tb.SummaryWriter(log_dir + '/{}'.format(time.strftime('%H-%M-%S')), flush_secs=1)\n",
    "    global_step = 0\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    # TODO: Choose a loss function\n",
    "    loss_fun = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        model.train()\n",
    "    \n",
    "        for image, label in data_train:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            pred = model(image)\n",
    "            loss_val = loss_fun(pred, label)\n",
    "            optim.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pdj = calculate_pdj(pred, label)\n",
    "            logger.add_scalar('train/pdj', float(pdj), global_step=global_step)\n",
    "            logger.add_scalar('train/loss', float(loss_val), global_step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # Compute the average pdj on the validation set\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        val_pdj = list()\n",
    "        for image, label in data_test:\n",
    "            with torch.no_grad():\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                pred = model(image)\n",
    "                pdj = calculate_pdj(pred, label)\n",
    "                val_pdj.append(pdj.item())\n",
    "        logger.add_scalar('val/pdj', torch.FloatTensor(val_pdj).mean().item(), global_step=global_step)\n",
    "\n",
    "        visualize(logger, image, pred, label, global_step)\n",
    "\n",
    "\n",
    "# Actually train the model here!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Network(3, 14)\n",
    "model.to(device)\n",
    "\n",
    "train(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev78V5rstq_o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coding_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
