{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1J7qTm1RinT"
   },
   "source": [
    "## Coding 7: UpConvs for Image Generation\n",
    "\n",
    "In this exercise, we'll get a breath of fresh air from CIFAR-10 and classification and actually do the opposite of multi-class classification. In multi-class classification, we take an image as input, and output a label.\n",
    "\n",
    "This week, we'll train a network that **takes in a label as input, and outputs an image**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*wy3oRM8jh8LKF6Iku2Uh4w.png\" width=1024px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS5ygM07xeFn"
   },
   "source": [
    "## TensorBoard Setup\n",
    "\n",
    "Same old embedded tensorboard code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8O91U4Qxg-O"
   },
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard as tb\n",
    "import tempfile\n",
    "\n",
    "log_dir = tempfile.mkdtemp()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} --reload_interval 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCZ8-pkWxk2w"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Here we'll use a custom dataset of the faces.\n",
    "\n",
    "**WARNING**: This cell will overwrite any directory called `images/`. If you have an `images/` directory in the folder this file is in, move it before running this cell.\n",
    "\n",
    "This cell relies on some Unix-like tools, but all it's doing is downloading and unzipping an archive. If you can't run this cell, just download the zip file from the given URL and unzip it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zudQV9Q6usIl"
   },
   "outputs": [],
   "source": [
    "!rm -f images.zip\n",
    "!rm -rf images/\n",
    "!wget 'https://docs.google.com/uc?export=download&id=1CWDIj3jdiAWINZXBlczhOMT3wImzlk1v' -O images.zip\n",
    "!unzip images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the people in this dataset are:\n",
    "\n",
    "- [An Wang](https://en.wikipedia.org/wiki/An_Wang)\n",
    "- [Héctor García-Molina](https://en.wikipedia.org/wiki/H%c3%a9ctor_Garc%c3%ada-Molina)\n",
    "- [Katherine Johnson](https://en.wikipedia.org/wiki/Katherine_Johnson)\n",
    "- [Marian Croak](https://en.wikipedia.org/wiki/Marian_Croak)\n",
    "- [Mark Dean](https://en.wikipedia.org/wiki/Mark_Dean_(computer_scientist))\n",
    "- [Raj Reddy](https://en.wikipedia.org/wiki/Raj_Reddy)\n",
    "- [Roy Clay Sr.](https://en.wikipedia.org/wiki/Roy_Clay)\n",
    "- [Sanghamitra Mohanty](https://en.wikipedia.org/wiki/Sanghamitra_Mohanty)\n",
    "- [Sophie Wilson](https://en.wikipedia.org/wiki/Sophie_Wilson)\n",
    "- [Xia Peisu](https://en.wikipedia.org/wiki/Xia_Peisu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppgc3qQ3Oyga"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Currently, the dataset yields pairs of (image, label), where label is an int (image id). Since our network will take the label as input, it's common practice to transform the label into a **one-hot vector**.\n",
    "\n",
    "If we have a total of $n$ labels, the one-hot representation of a label $l$ is a $n$-dimensional vector $x$, where $x_i$ = 1 if $i = l$, and $0$ otherwise.\n",
    "\n",
    "### One-hot example  \n",
    "if we have 5 labels, the one hot representation of the label 0 is `[1, 0, 0, 0, 0]`, the one hot representation of the label 1 is `[0, 1, 0, 0, 0]`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMTiu3UZTbDQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "images = torch.stack([to_tensor(Image.open(p).convert('RGB')) for p in sorted(pathlib.Path('images').glob('*.jpeg'))])\n",
    "labels = torch.eye(images.shape[0])\n",
    "# torch.eye creates the identity matrix with the given dimensions. Each row of the\n",
    "# matrix is a one-hot encoding for one image.\n",
    "\n",
    "vis = torchvision.utils.make_grid(images, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(30, 3))\n",
    "plt.imshow(vis.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vhhW0eCNiKX"
   },
   "source": [
    "## Model Implementation and Training\n",
    "\n",
    "Some questions to wonder about -\n",
    "\n",
    "* What network should we use?\n",
    "* what loss should we use?\n",
    "* Should we constrain the network output to [0, 1] like an image? How?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3JQBXDGXB6J"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class UpNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        # TODO: Define the model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Translate one-hot vector to image.\n",
    "        Remember that an image ranges from [0 .. 1].\n",
    "\n",
    "        Input: \n",
    "            x (float tensor N x 10): input id encoded as a one-hot vector\n",
    "        Output:\n",
    "            y (float tensor N x 3 x 256 x 256): image corresponding to each id\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def check_num_parameters(self):\n",
    "        \"\"\"\n",
    "        Get the total number of parameters in the network.\n",
    "        \"\"\"\n",
    "        return sum([np.prod(param.shape) for param in self.parameters()])\n",
    "\n",
    "\n",
    "def model_quality(model):\n",
    "    \"\"\"\n",
    "    Get a measure of the quality of the model.\n",
    "    \n",
    "    This measure is -log(x/y+e) where x is the sum of distances between the\n",
    "    generated images and the input images, y is the sum of the dstaances\n",
    "    between the input images and the image where every pixel is 0.5, and e is\n",
    "    some very small value (exp(-10)).\n",
    "    \n",
    "    Intuitively, as the generated images get close to the input images, x\n",
    "    becomes very small, and x/y approaches 0 from above. The denominator y\n",
    "    acts as a kind of normalizer, showing how a \"random\" image might score.\n",
    "    If x is zero (i.e., we reconstruct the images personally) then this\n",
    "    expression returns -log(e) = 10. As x grows, x/y grows, log(x/y) grows,\n",
    "    and -log(x/y) shrinks.\n",
    "    \"\"\"\n",
    "    return float(-(torch.sum((model(labels)-images)**2) / torch.sum((0.5-images)**2)+np.exp(-10)).log())\n",
    "\n",
    "def size_penalty(model):\n",
    "    \"\"\"\n",
    "    Get a penalty based on the size of the model.\n",
    "    \n",
    "    We want to learn small models since they are both faster to train and\n",
    "    faster to evaluation. This function gets a penalty based on the model size\n",
    "    and the size of the inputs.\n",
    "    \"\"\"\n",
    "    return 10 * model.check_num_parameters() / np.prod(images.shape)\n",
    "\n",
    "\n",
    "def train(model, device, lr=5e-3, iterations=5000):\n",
    "    # Setting up the tensorboard logger\n",
    "    logger = tb.SummaryWriter(log_dir + '/{}'.format(time.strftime('%H-%M-%S')), flush_secs=1)\n",
    "\n",
    "    # TODO: What loss should we use?\n",
    "    loss_function = None\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('size_penalty =', size_penalty(model))\n",
    "\n",
    "    for global_step in range(iterations):\n",
    "        if global_step % 100 == 0:\n",
    "            print(\"Iteration:\", global_step)\n",
    "        images_pred = model(labels)\n",
    "\n",
    "        loss = loss_function(images_pred, images)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # Add loss to TensorBoard.\n",
    "        if global_step % 10 == 0:\n",
    "            logger.add_scalar('loss', loss.item(), global_step=global_step)\n",
    "            logger.add_scalar('quality', model_quality(model), global_step=global_step)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            image_grid = (torchvision.utils.make_grid(images_pred, nrow=10) * 255).byte()\n",
    "            logger.add_image('image_pred', image_grid, global_step=global_step)\n",
    "\n",
    "    model.eval()\n",
    "    print('model_quality   = ', model_quality(model))\n",
    "    print('score           = ', model_quality(model)-size_penalty(model))\n",
    "\n",
    "# Actually train the model here!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UpNetwork(10, 3)\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "train(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDOaP3zj1Jti"
   },
   "source": [
    "## Model Evaluation and Visualization\n",
    "\n",
    "Let's see how our image generation model works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FociVwYU0oI1"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.cpu()\n",
    "image_pred = model(torch.eye(10))\n",
    "\n",
    "vis = torchvision.utils.make_grid(image_pred, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(30, 3))\n",
    "plt.imshow(vis.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wCZ8-pkWxk2w",
    "ppgc3qQ3Oyga"
   ],
   "name": "coding_07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
