{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUbWjm9pTqDY"
   },
   "source": [
    "# Coding 13: Reinforcement Learning\n",
    "\n",
    "This week we will train a deep network that learns to race in SuperTuxKart using RL or gradient free optimization.\n",
    "\n",
    "<img src=\"https://a.fsdn.com/con/app/proj/supertuxkart/screenshots/500px-Hac.jpg/max/max/1\" width=512px/>\n",
    "\n",
    "RL will likely not work on the image, or any state that is too high dimensional (within the timeframe of this class). In order to simplify the problem, we'll use some features extracted from the input by the original autopilot. In addition, we will only look at gradient-free optimization techniques in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq9eXWrmcVEk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "For this notebook you'll need `ray`, a parallelization package we will use to collect rollouts asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjYGw3YUU8G9"
   },
   "outputs": [],
   "source": [
    "!pip install PySuperTuxKart ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtOE8RW5cot8"
   },
   "source": [
    "## The Environment\n",
    "\n",
    "The following code wraps the `pystk` library and provides a basic interface for playing the game. This is the same code we used in last week's exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjYGtvDuTqDZ"
   },
   "outputs": [],
   "source": [
    "import pystk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class PyTux(object):\n",
    "    INITED = False\n",
    "\n",
    "    def __init__(self, track, screen_width=128, screen_height=96, use_graphics=True):\n",
    "        self.race = None\n",
    "        self.config = pystk.GraphicsConfig.hd() if use_graphics else pystk.GraphicsConfig.none()\n",
    "        self.config.screen_width = screen_width\n",
    "        self.config.screen_height = screen_height\n",
    "        self.track = track\n",
    "\n",
    "        if not PyTux.INITED:\n",
    "            pystk.init(self.config)\n",
    "            PyTux.INITED = True\n",
    "\n",
    "    @staticmethod\n",
    "    def _magical_auto_pilot(player, track, distance=20):\n",
    "        \"\"\"\n",
    "        This function return a magical steering, acceleration and drift values.\n",
    "        This is used in the auto-pilot and meant to be hard to read ;)\n",
    "        Feel free to get inspired by this if you can decipher it\n",
    "        (it's probably not worth your time though)\n",
    "        \"\"\"\n",
    "        __ = PyTux._point_on_track(player.kart.distance_down_track+distance, track)\n",
    "        __ = __ - np.array(player.kart.location)\n",
    "        _ = np.array(player.kart.front) - np.array(player.kart.location)\n",
    "        _ = _ / max(np.linalg.norm(_), 1e-10)\n",
    "        _ = np.cross([0,1,0], _)\n",
    "        return lambda ___: (_.dot(__), int(___<15), abs(__.dot(_))>1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _point_on_track(distance, track):\n",
    "        node_idx = np.searchsorted(track.path_distance[..., 1], distance % track.path_distance[-1, 1]) % len(track.path_nodes),\n",
    "        d = track.path_distance[node_idx]\n",
    "        x = track.path_nodes[node_idx]\n",
    "        t = (distance - d[0]) / (d[1] - d[0])\n",
    "        return x[1] * t + x[0] * (1 - t)\n",
    "\n",
    "    def clean(self):\n",
    "        if self.race is not None:\n",
    "            self.race.stop()\n",
    "            del self.race\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.clean()\n",
    "\n",
    "        config = pystk.RaceConfig(num_kart=1, laps=1, track=self.track, step_size=0.1)\n",
    "        config.players[0].controller = pystk.PlayerConfig.Controller.PLAYER_CONTROL\n",
    "\n",
    "        self.race = pystk.Race(config)\n",
    "        self.race.start()\n",
    "        self.race.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.clean()\n",
    "\n",
    "    def rollout(self, agent, max_frames=1000, use_image=False):\n",
    "        \"\"\"\n",
    "        agent: an object that implements the act method\n",
    "        max_frames: maximum number of frames to play for\n",
    "\n",
    "        returns: tuple of (number steps, overall distance, did the agent finish)\n",
    "        \"\"\"\n",
    "        state = pystk.WorldState()\n",
    "        track = pystk.Track()\n",
    "\n",
    "        time_bonus = 0\n",
    "\n",
    "        for t in range(max_frames):\n",
    "            state.update()\n",
    "            track.update()\n",
    "\n",
    "            player = state.players[0]\n",
    "\n",
    "            time_bonus = 100 * (max_frames - t - 1) / max_frames\n",
    "            # Terminate if the kart finishes a lap.\n",
    "            if np.isclose(player.kart.overall_distance / track.length, 1.0, atol=2e-3):\n",
    "                return t, 200 - 100 * (t + 1) / max_frames, True\n",
    "\n",
    "            # TODO: Compute features for our RL agent\n",
    "\n",
    "\n",
    "            loc = np.array(player.kart.location)\n",
    "            front_dir = np.array(player.kart.front) - loc\n",
    "            front_dir = front_dir / max(np.linalg.norm(front_dir), 1e-10)\n",
    "            left_dir = np.cross([0,1,0], front_dir)\n",
    "            kwargs = {\n",
    "                'feature': [left_dir.dot(np.array(PyTux._point_on_track(player.kart.distance_down_track+d, track))-loc) for d in [5, 10, 15, 20]],\n",
    "                'speed': np.linalg.norm(player.kart.velocity)\n",
    "            }\n",
    "            if use_image:\n",
    "                kwargs['image'] = np.array(self.race.render_data[0].image)\n",
    "            action = agent.act(**kwargs)\n",
    "            self.race.step(action)\n",
    "        return t, 100 * player.kart.distance_down_track / track.length + time_bonus, False\n",
    "\n",
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class RayPyTux(PyTux):\n",
    "    def __init__(self, track, screen_width=128, screen_height=96, use_graphics=True):\n",
    "        config = pystk.GraphicsConfig.hd() if use_graphics else pystk.GraphicsConfig.none()\n",
    "        config.screen_width = screen_width\n",
    "        config.screen_height = screen_height\n",
    "        pystk.init(config)\n",
    "        \n",
    "\n",
    "        race_config = pystk.RaceConfig(num_kart=1, laps=1, track=track, step_size=0.1)\n",
    "        race_config.players[0].controller = pystk.PlayerConfig.Controller.PLAYER_CONTROL\n",
    "\n",
    "        self.race = pystk.Race(race_config)\n",
    "        self.race.start()\n",
    "        self.race.step()\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        pass\n",
    "\n",
    "    def rollout(self, agent, max_frames=1000, use_image=False):\n",
    "        self.race.restart()\n",
    "        self.race.step()\n",
    "        return super().rollout(agent, max_frames, use_image)\n",
    "\n",
    "TRACK = 'lighthouse'\n",
    "TRACK_TIME = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s31WSaY7TqDb"
   },
   "source": [
    "## The Agent\n",
    "\n",
    "The agent wrapper is, once again, the same as last week's. The only change here is that we store some high-level features in addition to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDRPQRPgTqDc"
   },
   "outputs": [],
   "source": [
    "class AgentWrapper(object):\n",
    "    \"\"\"\n",
    "    Wraps any agent to collect extra information, used for\n",
    "    - collecting data\n",
    "    - visualizing runs\n",
    "    \"\"\"\n",
    "    ACTIONS = ['steer', 'acceleration', 'brake', 'drift']\n",
    "\n",
    "    def __init__(self, agent, noise=0):\n",
    "        self.agent = agent\n",
    "\n",
    "        self.images = list()\n",
    "        self.features = list()\n",
    "        self.actions = list()\n",
    "\n",
    "    def act(self, feature, speed, image=None):\n",
    "        action = self.agent.act(feature, speed)\n",
    "        if image is not None:\n",
    "            self.images.append(image.copy())\n",
    "        self.features.append((feature, speed))\n",
    "        self.actions.append([getattr(action, x) for x in self.ACTIONS])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "        Call on the last line of a cell to visualize the most recent run.\n",
    "        \"\"\"\n",
    "        from moviepy.editor import ImageSequenceClip\n",
    "        from IPython.display import display\n",
    "\n",
    "        display(ImageSequenceClip(self.images, fps=15).ipython_display(width=512, autoplay=True, loop=True, max_duration=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtmPfxGKXe1g"
   },
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsf0SfLTXkXh"
   },
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "log_dir = 'rl_log'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} --reload_interval 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhRDCQu4TqDg"
   },
   "source": [
    "## Policy Network\n",
    "\n",
    "Now we can finally define our model. The model takes in a set of computed features and the current speed of the kart and decides what action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb0mXA2nEv9i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, c_in=4):\n",
    "        super().__init__()\n",
    "        # Implement the policy network -- don't get fancy here. We can get good\n",
    "        # results with a _very_ simple policy network. You may want to use\n",
    "        # separate networks for steering and acceleration. I would recommend\n",
    "        # avoiding the drift and brake actions for now. Note that your network\n",
    "        # will be called with both the features and the current speed of the\n",
    "        # kart -- your network architecture should take this into account.\n",
    "        \n",
    "        # NOTE: c_in=4 is appropriate for the set of features that are tracked\n",
    "        # by the code above. If you want to, you are welcome to mess with that\n",
    "        # code to gather more or fewer features, and then you'll need to set\n",
    "        # c_in accordingly.\n",
    "\n",
    "    def forward(self, feature, speed):\n",
    "        # TODO\n",
    "\n",
    "    def act(self, feature, speed, image=None):\n",
    "        # TODO: Given a set of features and a speed, take an action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pKbBCuyE52r"
   },
   "source": [
    "## Training\n",
    "\n",
    "The first training approach we'll try is the simplest. We'll just sample random policies and then keep track of the one which performs the best. This will work best if your model has few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkOGoUbXTqDg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def train_v0(model, device, epochs=100):\n",
    "    # logger = tb.SummaryWriter(log_dir + '/{}'.format(time.strftime('%H-%M-%S')), flush_secs=1)\n",
    "    env = RayPyTux.remote(TRACK, use_graphics=False)\n",
    "    best_model = deepcopy(model)\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        # TODO: Get a random model -- don't overthink this!\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            agent = AgentWrapper(model)\n",
    "            _, reward, _ = ray.get(env.rollout.remote(agent, use_image=False, max_frames=200))\n",
    "        \n",
    "        # logger.add_scalar('train/reward', reward, global_step=epoch)\n",
    "        \n",
    "        # TODO: If the reward is better than the best reward seen so far, store\n",
    "        # the current model in best_model\n",
    "        \n",
    "        print(f'Current Reward: {reward:6.2f}  (best {best_reward:6.2f})')\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Train your model.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Policy()\n",
    "\n",
    "model = train_v0(model, device, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can use the cell at the bottom of this notebook to see what your controller is doing.\n",
    "\n",
    "Now let's try something slightly more interesting. Starting from the best policy we found using `train_v0`, we'll sample some random perturbations and apply those perturbations to the policy parameters. We'll keep track of the best-performing result and return it. By doing this multiple times, we can \"walk\" through the parameter space to better and better models. The `train_v1` function performs one step of this process -- it samples some number of perturbations near a given policy and returns the best one. We can then use `train_v1` repeatedly to train the policy. You may find it useful to change the noise scale over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruhhiFElxKqk"
   },
   "outputs": [],
   "source": [
    "def train_v1(model, device, epochs=10, noise_scale=1):\n",
    "    # logger = tb.SummaryWriter(log_dir + '/{}'.format(time.strftime('%H-%M-%S')), flush_secs=1)\n",
    "    env = RayPyTux.remote(TRACK, use_graphics=False)\n",
    "    original_parameters = [p.data.clone() for p in model.parameters()]\n",
    "    best_model = deepcopy(model)\n",
    "    # TODO: Make sure you check the reward of the input model\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        # Sample a random perturbation and apply it to the model\n",
    "        for op, p in zip(original_parameters, model.parameters()):\n",
    "            noise = noise_scale * torch.randn(p.data.shape)\n",
    "            p.data[...] = op + noise\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            agent = AgentWrapper(model)\n",
    "            _, reward, _ = ray.get(env.rollout.remote(agent, use_image=False, max_frames=500))\n",
    "        \n",
    "        print(reward)\n",
    "        \n",
    "    # Restore the original parameters to the model\n",
    "    for op, p in zip(original_parameters, model.parameters()):\n",
    "        p.data[...] = op\n",
    "        \n",
    "    return best_model\n",
    "\n",
    "# Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7uo3IVygfeU"
   },
   "source": [
    "## Fully Autonomous Driving\n",
    "\n",
    "Now that all that has been taken care of, it's time to finally test out your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ9s-6RhTqDk"
   },
   "outputs": [],
   "source": [
    "# Test out your model!\n",
    "with PyTux(TRACK) as env:\n",
    "    agent = AgentWrapper(better_model)\n",
    "    with torch.no_grad():\n",
    "        print('Time: %d Score: %.2f Success: %d' % env.rollout(agent, use_image=True, max_frames=600))\n",
    "\n",
    "agent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M87JF8ztxOHb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Coding_13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
