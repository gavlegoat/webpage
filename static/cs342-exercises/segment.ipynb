{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1J7qTm1RinT"
   },
   "source": [
    "## Coding 8: Binary Segmentation\n",
    "\n",
    "In this exercise, we'll do segmentation (per-pixel classification).\n",
    "\n",
    "We'll use a dataset of horses with their corresponding masks,  \n",
    "and the goal is to train a network that outputs if a pixel is a horse or not.\n",
    "\n",
    "<img src=\"http://cvlab.postech.ac.kr/research/deconvnet/images/overall.png\" width=512px/>\n",
    "\n",
    "Image: Learning Deconvolution Network for Semantic Segmentation, Noh et al. ICCV 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpQYqjP8fW96"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Run this cell to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zudQV9Q6usIl"
   },
   "outputs": [],
   "source": [
    "!rm -f weizmann_horse_db.zip\n",
    "!rm -rf weizmann_horse_db/\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1Tj7M8maC3QDkAf_dDtTFFUvtyzcuhlkg\n",
    "!unzip -qq weizmann_horse_db.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppgc3qQ3Oyga"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset yields pairs of (image, label)\n",
    "\n",
    "The image is a [3, 128, 128] float tensor of a horse.  \n",
    "The label is a [128, 128] 0-1 float tensor of the mask of the horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMTiu3UZTbDQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class WeizmannHorseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.images = list()\n",
    "        self.masks = list()\n",
    "        if not transform:\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(128),\n",
    "                torchvision.transforms.CenterCrop(128),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        images = sorted((image_dir / 'horse').glob('*.png'))\n",
    "        masks = sorted((image_dir / 'mask').glob('*.png'))\n",
    "\n",
    "        for i, (image_path, mask_path) in enumerate(zip(images, masks)):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image.load()\n",
    "\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "            mask.load()\n",
    "            \n",
    "            self.images.append(image)\n",
    "            self.masks.append(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.transform(self.images[idx]), self.transform(self.masks[idx])\n",
    "        image = torchvision.transforms.functional.to_tensor(image)\n",
    "        mask = torch.tensor(np.asarray(mask, np.float32)[None])\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def fetch_dataloader(batch_size, transform=None, split='train'):\n",
    "    \"\"\"\n",
    "    Loads data from disk and returns a data_loader.\n",
    "    A DataLoader is similar to a list of (image, label) tuples.\n",
    "    You do not need to fully understand this code to do this assignment, we're happy to explain though.\n",
    "    \"\"\"\n",
    "    data = WeizmannHorseDataset(pathlib.Path('weizmann_horse_db'), transform=transform)\n",
    "\n",
    "    # Custom train/val split.\n",
    "    if split == 'train':\n",
    "        indices = [i for i in range(len(data)) if (i%10 > 1)]\n",
    "    elif split == 'val':\n",
    "        indices = [i for i in range(len(data)) if (i%10 == 1)]\n",
    "    else:\n",
    "        indices = [i for i in range(len(data)) if (i%10 == 0)]\n",
    "\n",
    "    data = torch.utils.data.Subset(data, indices)\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=split=='train', num_workers=0, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "data_train = fetch_dataloader(32, split='train')\n",
    "data_val = fetch_dataloader(32, split='val')\n",
    "data_test = fetch_dataloader(128, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DmxRAP9f31b"
   },
   "outputs": [],
   "source": [
    "# Setup the evaluation metric\n",
    "class Metric:\n",
    "    \"\"\"\n",
    "       Compute two metrics of success.\n",
    "       Average accuracy and intersection over union (also called Jaccard index)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.count, self.correct, self.intersection, self.union = 0, 0, 0, 0\n",
    "\n",
    "    def add(self, pred, label):\n",
    "        pred = pred > 0\n",
    "        label = label > 0\n",
    "        self.intersection += float((pred & label).float().sum())\n",
    "        self.union += float((pred | label).float().sum())\n",
    "        self.count += float((pred == pred).float().sum())\n",
    "        self.correct += float((label == pred).float().sum())\n",
    "    \n",
    "    @property\n",
    "    def iou(self):\n",
    "        return self.intersection / max(self.union, 1)\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self.correct / max(self.count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZKXI7tnJtJG"
   },
   "outputs": [],
   "source": [
    "def visualize(data):\n",
    "    images, masks = next(iter(data))\n",
    "    masks = masks.tile((1,3,1,1))\n",
    "\n",
    "    vis = torchvision.utils.make_grid([*images[:4], *masks[:4]], nrow=4)\n",
    "\n",
    "    plt.imshow(vis.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize(data_train)\n",
    "visualize(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsjUZNGoNbJs"
   },
   "source": [
    "## TensorBoard Setup\n",
    "\n",
    "Same old embedded tensorboard code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaoyJOLrNaff"
   },
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard as tb\n",
    "import tempfile\n",
    "\n",
    "log_dir = tempfile.mkdtemp()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} --reload_interval 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vhhW0eCNiKX"
   },
   "source": [
    "## Model and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3JQBXDGXB6J"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is the main network for this task.\n",
    "    \n",
    "    Note that since we are looking at a segmentation task with only two classes,\n",
    "    we are using a slightly different output format than we have looked at\n",
    "    before. We will only output a single value per pixel, and if the value is\n",
    "    less than zero, we classify that pixel as 'background', if it's greater than\n",
    "    zero we classify that pixel as 'horse'. This reduces the complexity of\n",
    "    training slightly since we only need to produce one output per pixel rather\n",
    "    than two.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=3, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Conv2d(3, 1, 5, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Translate the image to a mask of the horse.\n",
    "\n",
    "        Input: \n",
    "            x (float tensor N x 3 x 128 x 128): input image of a horse\n",
    "        Output:\n",
    "            y (float tensor N x 1 x 128 x 128): binary mask of the horse\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train(model, lr=1e-4, epochs=50):\n",
    "    # Setting up the tensorboard logger\n",
    "    logger = tb.SummaryWriter(log_dir + '/{}'.format(time.strftime('%H-%M-%S')), flush_secs=1)\n",
    "    global_step = 0\n",
    "\n",
    "    # Pick a loss and optimizer\n",
    "    # We haven't seen BCEWithLogits before -- it is similar to CrossEntropyLoss,\n",
    "    # but it is designed for binary tasks.\n",
    "    loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "    optim = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        # Train for an epoch\n",
    "        model.train()\n",
    "        metric = Metric()\n",
    "        for image, label in data_train:\n",
    "            # Move image, label to GPU\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            \n",
    "            # Compute network output\n",
    "            pred = model(image)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_val = loss_fun(pred, label)\n",
    "\n",
    "            metric.add(pred, label)\n",
    "\n",
    "            # Zero gradient\n",
    "            optim.zero_grad()\n",
    "            # Backward\n",
    "            loss_val.backward()\n",
    "            # Step optim\n",
    "            optim.step()\n",
    "            # Logging\n",
    "            logger.add_scalar('train/loss', float(loss_val), global_step=global_step)\n",
    "            global_step += 1\n",
    "        \n",
    "        logger.add_scalar('train/accuracy', float(metric.accuracy), global_step=global_step)\n",
    "        logger.add_scalar('train/iou', float(metric.iou), global_step=global_step)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        metric = Metric()\n",
    "        for it, (image, label) in enumerate(data_val):\n",
    "            # Move image, label to GPU\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            \n",
    "            # Compute network output\n",
    "            pred = model(image)\n",
    "\n",
    "            metric.add(pred, label)\n",
    "\n",
    "            if it == 0:\n",
    "                logger.add_image('val/image', torchvision.utils.make_grid(image[:8], nrow=4), global_step=global_step)\n",
    "                logger.add_image('val/pred', torchvision.utils.make_grid(torch.sigmoid(pred[:8]), nrow=4), global_step=global_step)\n",
    "                logger.add_image('val/output', torchvision.utils.make_grid((pred[:8]>0).float(), nrow=4), global_step=global_step)\n",
    "        logger.add_scalar('val/accuracy', float(metric.accuracy), global_step=global_step)\n",
    "        logger.add_scalar('val/iou', float(metric.iou), global_step=global_step)\n",
    "\n",
    "\n",
    "# Actually train the model here!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Network(3, 1)\n",
    "model.to(device)\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M2ucIIoNEI0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "in_class_coding_08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
