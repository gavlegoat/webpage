{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Data\n",
    "\n",
    "In this notebook we will look at how patterns emerge even in completely random data, especially when the data is many-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "cls = linear_model.SGDClassifier(max_iter=1000)\n",
    "\n",
    "# Some sklearn versions spam warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will generate a number of datapoints with a given dimension, then split that data completely randomly into two separate classes. It will then fit a linear classifier to the data. Intuitively, because the data is generated and split randomly, we wouldn't expect a linear classifier to be able to separate the two classes. However, as we will see, when the dimension of the data is large, the classifier will be able to find features which separate the two classes, even though those classes have no semantic meaning to humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_points(N=2000, d=2):\n",
    "    \n",
    "    # Generate N random d-dimensional data points\n",
    "    X = np.random.normal(0,1,size=(N, d))\n",
    "    # Separate the data randomly into two classes where the data is in class\n",
    "    # 0 with probability prob and class 1 with probability 1-prob.\n",
    "    prob = 0.5\n",
    "    y = np.random.rand(N) >= prob\n",
    "\n",
    "    # Fit the first classifier\n",
    "    cls.fit(X, y)\n",
    "    # Let's project all points along the decision boundary of the first classifier\n",
    "    p1 = cls.coef_ / np.linalg.norm(cls.coef_)\n",
    "    d1 = X.dot(p1.T)\n",
    "    score = cls.score(X, y)\n",
    "\n",
    "    # Fit a second classifier on the projected points.\n",
    "    cls.fit(X - d1*p1, y)\n",
    "    p2 = cls.coef_ / np.linalg.norm(cls.coef_)\n",
    "    d2 = X.dot(p2.T)\n",
    "\n",
    "    # Plot the two classifiers\n",
    "    figure(figsize=(10,5))\n",
    "    subplot(1,2,1)\n",
    "    # Plot the data projected onto it's first two dimensions\n",
    "    scatter(*X[:,:2].T, c=y.flat, cmap='Paired')\n",
    "    axis('equal')\n",
    "    subplot(1,2,2)\n",
    "    # Now plot the projection of the data long the axes defined by the two classifiers.\n",
    "    scatter(d1.flat, d2.flat, c=y.flat, cmap='Paired')\n",
    "    axis('equal')\n",
    "\n",
    "    print( score )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at some low-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_points(2000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see random-looking clouds of points in both images. This is what we intuitively expect when we generate random data. But now let's see what happens when we increase the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_points(4000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the second figure, the two classes are starting to separate. One class is clearly more heavily represented on the left and the other on the right. This is because even though the data is still totally random, the classifier is starting to overfit. You'll also notice that the accuracy of the linear classifier, shown above the graphs, is significantly higher than 0.5.\n",
    "\n",
    "In the following cell, we go to an even higher dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_points(4000, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have as many dimensions as we have data points, and a linear classifier is able to almost perfectly separate the data.\n",
    "\n",
    "In the following cell, we plot the accuracy of linear classifiers over datasets of of different sizes, where the data has different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sep(d=2, N=[10,25,50,100,250,500,1000,2000]):\n",
    "    S = []\n",
    "    mnS, mxS = [], []\n",
    "    for n in N:\n",
    "        s = []\n",
    "        for it in range(5000 // n + 1):\n",
    "            X = np.random.normal(0,1,size=(n, d))\n",
    "            y = np.random.rand(n)\n",
    "            y = y > np.median(y)\n",
    "\n",
    "            # Fit the first classifier\n",
    "            s.append( cls.fit(X, y).score(X,y) )\n",
    "        S.append(np.mean(s))\n",
    "#         mnS.append(np.min(s))\n",
    "#         mxS.append(np.max(s))\n",
    "        mnS.append(np.mean(s) - np.std(s))\n",
    "        mxS.append(np.mean(s) + np.std(s))\n",
    "    fill_between(N, mnS, mxS, alpha=0.5)\n",
    "    plot(N,S, label='d = %d'%d)\n",
    "    xlabel('# data points')\n",
    "    ylabel('classification accuracy')\n",
    "    xscale('log')\n",
    "    legend()\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "plot_sep(2)\n",
    "plot_sep(10)\n",
    "plot_sep(100)\n",
    "plot_sep(1000)\n",
    "plot_sep(32*32*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chart, the different colored lines represent datasets in different dimensions. For each dimension, we generate datasets of different sizes and fit a linear classifier. We also repeat each experiment several times -- the line shows the mean accuracy over all of the experiments and the shaded envelope shows one standard deviation on each side of the mean. Notice that in higher dimensions, the accuracy of the linear classifiers is very high until the number of data points becomes large. In fact, when the number of dimensions is greater than the number of datapoints, the linear classifier *perfectly* separates the two classes, even though the data are completely random. In practice, the datasets we work with in computer vision are very high-dimension. For example, a well-known classification network, ResNet18, takes 224x224 images as inputs. Lets see how many dimensions that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "224*224*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for images of this size, we would still need more than 150,000 data points just to prevent a linear separator from *perfectly* separating randomly distributed data. This is why overfitting happens -- it is simply unavoidable that there will be exploitable patterns in the data, especially in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
