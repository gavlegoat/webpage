{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5f9837-9a7c-4aa9-af04-942b87b70780",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in PyTorch\n",
    "\n",
    "In the in-class exercise this week, we are looking at how to do imitaiton learning in PyTorch. In this notebook, we will look at two other approaches to reinforcement learning: REINFORCE, a basic policy gradient algorithm, and augmented random search, a gradient-free approach.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Most reinforcement learning implementations use the `gym` package to represent environments, so we will do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dbf49-79c9-4202-9493-162cd1412830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addb0dc-bfb1-4668-9b2e-c91cdabb234b",
   "metadata": {},
   "source": [
    "To keep things a little simpler, we will not use the SuperTuxKart environment for this notebook. Instead, we will use the classic CartPole environment. In this setting, we are controlling a cart which is able to move left and right along a track. There is a pole attached to the cart, pointing upward, which can swing freely at it's hinge. The goal of the environment is to move the cart in order to keep the pole upright (like trying to balance a stick on your hand). The CartPole environment comes in two varieties: one with a discrete action space (at every time step you have two options: move left or move right) and one with a continuous action space (at every time step you can apply a real-valued force to the cart). We will implement the continuous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b21df-4c8d-4506-9532-ffc93d04ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the code is taken from the gym implementation of the discrete\n",
    "# version of cartpole available at:\n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "class CartPole(gym.Env):\n",
    "    def __init__(self):\n",
    "        # We set up a bunch of parameters of the environment here\n",
    "        self.gravity = 9.8     # Normal gravitational constant\n",
    "        self.masscart = 1.0    # Mass of the cart\n",
    "        self.masspole = 0.1    # Mass of the pole\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5      # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0  # Maximum amount of force which can be applied\n",
    "        self.dt = 0.02         # Time step\n",
    "        \n",
    "        # Failure conditions: if the pole reaches 20 degrees from vertical\n",
    "        # or if the cart deviates 2.4 meters from origin.\n",
    "        self.theta_threshold = 20 * np.pi / 180\n",
    "        self.x_threshold = 2.4\n",
    "        \n",
    "        # Reinforcement learning environments must specify an action space\n",
    "        # which the agent can choose from and an observation space which the\n",
    "        # agent should expect as input. In this case, both are in the Box\n",
    "        # class, meaning they are continuous spaces with independent bounds\n",
    "        # on each dimension.\n",
    "        # The action space is [-1, 1], indicating how much force to apply\n",
    "        self.action_space = gym.spaces.Box(-np.array([1]), np.array([1]), dtype=np.float32)\n",
    "        # The observation space is 4-dimensional: The position and velocity of\n",
    "        # the cart, and the angle and angular velocity of the pole.\n",
    "        high = np.array([\n",
    "            self.x_threshold,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold,\n",
    "            np.finfo(np.float32).max\n",
    "        ])\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "        \n",
    "        # Once we start the environment, the state will be stored here\n",
    "        self.state = None\n",
    "        \n",
    "    # All environments should define a reset function which puts the\n",
    "    # environment back into an initial state.\n",
    "    def reset(self):\n",
    "        self.state = -0.05 + 0.1 * np.random.rand(4)\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "    \n",
    "    # The step function defines how the environment responds to actions. It\n",
    "    # takes an action as input and returns 4 things:\n",
    "    # - The new state\n",
    "    # - Whether or not the episode is done\n",
    "    # - The reward\n",
    "    # - A dictionary with any extra information\n",
    "    def step(self, action):\n",
    "        # For this environment, we do some physics to see what the new state\n",
    "        # should be.\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag * action[0]\n",
    "        \n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        x = x + self.dt * x_dot\n",
    "        x_dot = x_dot + self.dt * xacc\n",
    "        theta = theta + self.dt * theta_dot\n",
    "        theta_dot = theta_dot + self.dt * thetaacc\n",
    "        \n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        # In this environment, we are done if we exceed the threshold on\n",
    "        # either the horizontal position or the angle of the pole.\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold\n",
    "            or theta > self.theta_threshold\n",
    "        )\n",
    "\n",
    "        # The reward for the cartpole environment is simply 1 for every\n",
    "        # timestep where the pole doesn't fall over. This means the best agent\n",
    "        # is the one which can keep the pole balanced for the longest.\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "    \n",
    "    # An environment may also optionally define a 'render' method which can be\n",
    "    # used for displaying rollouts. We will not be doing that in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9598b4-4594-4bc4-8ea0-230d6694dc08",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "Now that we have an environment defined, let's implement REINFORCE. The first thing we need is a trainable agent. The important thing to note here is that the agent is going to return a *stochastic* action. That means we aren't predicting an action direction -- rather we are predicting the parameters of a distribution which we can sample actions from. To do this, we can use the `torch.distributions` library, and we will ask the network to predict the mean and standard deviation of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc68a1-76ed-44a2-8f23-eacfab90d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Let's start with a really simple linear agent.\n",
    "        self.net = torch.nn.Linear(4, 2).to(device)\n",
    "        \n",
    "    # We allow the agent to produce either stochastic or deterministic\n",
    "    # actions. It's pretty common to use stochastic actions during training\n",
    "    # time then switch to deterministic actions for testing. In our case,\n",
    "    # if we want a deterministic action we can use the mean prediction.\n",
    "    def __call__(self, state, deterministic=False):\n",
    "        # We need to do some reshaping in order to feed a single state through\n",
    "        # our network.\n",
    "        params = self.net(torch.as_tensor(state).view(1, -1).to(device))[0]\n",
    "        if deterministic:\n",
    "            action = params[0].view(1)\n",
    "        else:\n",
    "            distr = distributions.normal.Normal(params[0], torch.sigmoid(params[1]))\n",
    "            action = distr.sample(sample_shape=(1,))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b9425-3350-4d0a-a43a-53d213c87e82",
   "metadata": {},
   "source": [
    "Next, we can move on the the actual training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d54238-9721-43ce-97b4-5b69f4eb87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# First, we set up some hyperparameters\n",
    "epochs = 20                       # Number of epochs to train for\n",
    "episodes_per_epoch = 100          # Number of trajectories per epoch\n",
    "max_episode_len = 50              # Maximuim length of a trajectory\n",
    "batch_size = 128                  # Batch size for network updates\n",
    "policy_updates_per_epoch = 50     # Number of times to update the policy\n",
    "gamma = 0.99                      # Discount factor\n",
    "test_episodes = 20                # Episodes to use for testing (validation)\n",
    "\n",
    "# We also need the environment and agent\n",
    "env = CartPole()\n",
    "agent = Agent()\n",
    "\n",
    "# We create an optimizer as normal.\n",
    "optim = torch.optim.Adam(agent.net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # We use these lists to collect trajectories\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for ep in range(episodes_per_epoch):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        # These lists collect a single trajectory\n",
    "        ep_rewards = []\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        while not done:\n",
    "            # For training episodes, we use deterministic=False to get\n",
    "            # stochastic actions\n",
    "            action = agent(state, deterministic=False)\n",
    "            ep_states.append(state)\n",
    "            ep_actions.append(action)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ep_rewards.append(reward)\n",
    "            steps += 1\n",
    "            if steps >= max_episode_len:\n",
    "                done = True\n",
    "        \n",
    "        # Convert rewards into discounted returns. Taken from\n",
    "        # github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py\n",
    "        rs = []\n",
    "        R = 0\n",
    "        for r in ep_rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            rs.insert(0, R)\n",
    "        rs = torch.as_tensor(rs).to(device)\n",
    "        # It's useful to normalize the returns. Here we simply subtract the mean then\n",
    "        # divide by the standard deviation. The addition in the denominator just avoids\n",
    "        # divide-by-zero errors.\n",
    "        rewards.append((rs - rs.mean()) / (rs.std() + np.finfo(np.float32).eps.item()))\n",
    "        states.append(ep_states)\n",
    "        actions.append(ep_actions)\n",
    "        \n",
    "    # Now we flatten states, actions and rewards. This is slightly awkward just\n",
    "    # becasue the different items of these lists can have different lengths.\n",
    "    tmp_states = []\n",
    "    tmp_actions = []\n",
    "    tmp_rewards = []\n",
    "    for i in range(episodes_per_epoch):\n",
    "        for j in range(len(states[i])):\n",
    "            tmp_states.append(states[i][j])\n",
    "            tmp_actions.append(actions[i][j])\n",
    "            tmp_rewards.append(rewards[i][j].item())\n",
    "            \n",
    "    # Move everything to the GPU if necessary.\n",
    "    states = torch.as_tensor(tmp_states).to(device)\n",
    "    actions = torch.as_tensor(tmp_actions).to(device)\n",
    "    rewards = torch.as_tensor(tmp_rewards).to(device)\n",
    "    \n",
    "    agent.net.train()\n",
    "            \n",
    "    # Now we actually make updates to the network\n",
    "    for it in range(policy_updates_per_epoch):\n",
    "        # Randomly sample a batch of data from the collected trajectories\n",
    "        batch_idx = torch.randint(0, len(rewards), (batch_size,)).to(device)\n",
    "        batch_states = states[batch_idx]\n",
    "        batch_actions = actions[batch_idx]\n",
    "        batch_rewards = rewards[batch_idx]\n",
    "        \n",
    "        # Here we compute the parameters of the distribution which the network\n",
    "        # produces at each state and set up the distribution.\n",
    "        outputs = agent.net(batch_states)\n",
    "        distr = distributions.normal.Normal(outputs[:,0], torch.sigmoid(outputs[:,1]))\n",
    "        \n",
    "        # Recall that want to use the gradient E[grad log(pi(a | s)) * R(s, a)].\n",
    "        # log_prob gives is the log of the probability of the given actions\n",
    "        # based on the distribution parameters. We multiply this by the\n",
    "        # rewards in order to get the expected return to maximize.\n",
    "        expected_log_return = (distr.log_prob(batch_actions) * batch_rewards).mean()\n",
    "        optim.zero_grad()\n",
    "        # Now since we want to maximize the objective and Adam is set up to\n",
    "        # minimize, we just take the negative.\n",
    "        (-expected_log_return).backward()\n",
    "        optim.step()\n",
    "        \n",
    "    # Now we measure the performance\n",
    "    agent.net.eval()\n",
    "    ep_rewards = []\n",
    "    for ep in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            # In this part we use deterministic=True since we are testing the\n",
    "            # network rather than collecting training data. You can think of\n",
    "            # this as similar to how we treat dropout, where we add some\n",
    "            # randomness at training time but remove it for testing.\n",
    "            action = agent(state, deterministic=True)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            steps += 1\n",
    "            if steps >= max_episode_len:\n",
    "                done = True\n",
    "        ep_rewards.append(ep_reward)\n",
    "    print(\"Average reward:\", np.mean(ep_rewards))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999427a-27e2-4b46-91b2-3dc0613ea44c",
   "metadata": {},
   "source": [
    "If everything went well, you should see that the average reward increases with each epoch. Note that if the average reward reaches `max_episode_len` then the pole never falls over within the episode window, so your reward is as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8c590-ee61-43a1-b721-ffd7e3af956b",
   "metadata": {},
   "source": [
    "## Augmented Random Search\n",
    "\n",
    "Now let's move on and look at a gradient free approach to this same problem. There are a number of interesting gradient free algorithms out there, but we will use augmented random search, as a relatively new (2018) and high-performance approach. First, we'll define an agent class as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78dbd1-db8f-4e36-8dd2-ffec7d0729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Once again, we'll use a simple linear network. In this case we will\n",
    "        # always be predicting actions directly rather than distribution\n",
    "        # parameters.\n",
    "        self.network = torch.nn.Linear(4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "# We will also want a way to get and set a vector with all of the parameters\n",
    "# of the agent.\n",
    "def get_flat_params(agent):\n",
    "    return np.concatenate([p.data.cpu().numpy().flatten() for p in agent.parameters()])\n",
    "\n",
    "def set_flat_params(agent, v):\n",
    "    i = 0\n",
    "    for p in agent.parameters():\n",
    "        n = np.prod(p.data.size())\n",
    "        p.data[:] = torch.as_tensor(v[i:i+n].reshape(p.data.size()), dtype=p.dtype, device=p.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d0cfb-d87a-481d-80a6-1e28c9113d1f",
   "metadata": {},
   "source": [
    "Now for the training code. In this case, we won't be computing gradients to use with an optimizer. Instead, we will set the parameters manually using the two functions we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692758d0-04b3-4c6a-8233-9ce4c43d8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iters controls the total number of parameter updates to perform. I've found\n",
    "# that this method can vary quite a bit in how many iterations are necessary --\n",
    "# sometimes it finds an optimum in 50 iterations, sometimes it takes several\n",
    "# hundred.\n",
    "iters = 500\n",
    "iters_per_evaluation = 20  # How frequently to print an evaluation\n",
    "steps_per_epoch = 10       # number of deviations to measure per iteration\n",
    "max_episode_len = 50       # maximum length of a trajectory\n",
    "test_episodes = 20         # Number of episodes to use for testing\n",
    "eps = 0.1                  # std dev. of random perturbations\n",
    "alpha = 1.0                # learning rate\n",
    "\n",
    "# As before, we set up an agent and an environment\n",
    "agent = Agent2()\n",
    "env = CartPole()\n",
    "\n",
    "# This is a utility function to perform one episode and return the total reward.\n",
    "@torch.no_grad()\n",
    "def rollout(agent, env, horizon=50):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    ep_reward = 0.0\n",
    "    while not done:\n",
    "        action = agent(torch.as_tensor(state).view(1, -1))[0].numpy()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        steps += 1\n",
    "        if steps >= horizon:\n",
    "            break\n",
    "    return ep_reward\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in tqdm(range(iters)):\n",
    "        p = get_flat_params(agent)\n",
    "        all_grads = []\n",
    "        grads = []\n",
    "        for i in range(steps_per_epoch):\n",
    "            # We sample a bunch of deviations from a normal distribution\n",
    "            dp = np.random.normal(0, eps, p.shape)\n",
    "            # For each deviation, calculate the reward of p + dp and p - dp\n",
    "            set_flat_params(agent, p + dp)\n",
    "            score_pos = rollout(agent, env, horizon=max_episode_len)\n",
    "            set_flat_params(agent, p - dp)\n",
    "            score_neg = rollout(agent, env, horizon=max_episode_len)\n",
    "            # We keep track of these scores in two lists\n",
    "            grads.append((dp, score_pos, score_neg))\n",
    "            # all_grads is just for convenience for computing the standard\n",
    "            # deviation of scores.\n",
    "            all_grads.append(score_pos)\n",
    "            all_grads.append(score_neg)\n",
    "        \n",
    "        # This is the update rule for augmented random search. It is\n",
    "        # essentially using the random deviations we looked at as approximations\n",
    "        # to a gradient.\n",
    "        dp = alpha / np.std(all_grads) * np.sum([(pos - neg) * d for d, pos, neg in grads])\n",
    "        set_flat_params(agent, p + dp)\n",
    "    \n",
    "        # Evaluate the policy every so often\n",
    "        if (it + 1) % iters_per_evaluation == 0:\n",
    "            ep_rewards = []\n",
    "            for ep in range(test_episodes):\n",
    "                ep_rewards.append(rollout(agent, env, horizon=max_episode_len))\n",
    "            print(\"Average reward:\", np.mean(ep_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911591a6-c815-460e-a122-a73bd4cd9802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
